Vorgehen um die Notebooks auf dem Spark-Cluster ausführen zu können:

1. Daten entpacken
Entzippe data.zip und extrahiere die CSV-Dateien in das Verzeichnis data/.

2. Dateien hochladen
Lade die entpackten CSVs und die Jupyter Notebooks in den Spark-Cluster der ZHAW.

3. Notebooks ausführen
Teil 1: Datenaufbereitung (Teil-1-Preprocessing.ipynb) zuerst ausführen.

Teile 2-4 können danach unabhängig voneinander ausgeführt werden:
Teil-2-MapReduce-Analyse.ipynb
Teil-3-sparkSQL-Analyse.ipynb
Teil-4-Prophe.ipynb

Viel Erfolg!